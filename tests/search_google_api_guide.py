#!/usr/bin/env python
"""
ä½¿ç”¨ Crawl4AI MCP Server æœç´¢å¦‚ä½•è·å¾— Google API key å’Œ CSE ID
"""

import asyncio
import json
import sys
import os

# Add the src directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from search import SearchManager
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from crawl4ai import CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator


async def search_google_api_setup():
    """æœç´¢ Google API key å’Œ CSE ID çš„è·å–æ–¹æ³•"""
    
    print("=" * 80)
    print("æœç´¢ï¼šå¦‚ä½•è·å¾— Google API Key å’Œ CSE ID")
    print("=" * 80)
    print()
    
    # Initialize search manager
    search_manager = SearchManager()
    
    # æœç´¢æŸ¥è¯¢
    queries = [
        "how to get Google Custom Search API key",
        "Google CSE ID setup tutorial",
        "Google Custom Search Engine API configuration"
    ]
    
    all_results = {}
    
    for query in queries:
        print(f"ğŸ” æœç´¢: '{query}'")
        print("-" * 80)
        
        try:
            results = await search_manager.search(
                query, 
                num_results=3, 
                engine="duckduckgo"
            )
            
            if results:
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ\n")
                
                for i, result in enumerate(results, 1):
                    print(f"  ç»“æœ #{i}")
                    print(f"  æ ‡é¢˜: {result['title']}")
                    print(f"  URL: {result['link']}")
                    snippet = result['snippet']
                    if len(snippet) > 150:
                        snippet = snippet[:150] + "..."
                    print(f"  æ‘˜è¦: {snippet}")
                    print()
                
                all_results[query] = results
            else:
                print("âŒ æœªæ‰¾åˆ°ç»“æœ\n")
                
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {str(e)}\n")
    
    # æå–æœ€ç›¸å…³çš„URLè¿›è¡Œè¯¦ç»†å†…å®¹æå–
    print()
    print("=" * 80)
    print("æå–è¯¦ç»†å†…å®¹")
    print("=" * 80)
    print()
    
    # é€‰æ‹©æœ€ç›¸å…³çš„URL
    target_urls = []
    for query_results in all_results.values():
        for result in query_results[:2]:  # æ¯ä¸ªæŸ¥è¯¢å–å‰2ä¸ªç»“æœ
            url = result['link']
            if url not in target_urls and 'google' in url.lower():
                target_urls.append(url)
    
    # é™åˆ¶åªæå–å‰3ä¸ªURL
    target_urls = target_urls[:3]
    
    extracted_contents = {}
    
    for url in target_urls:
        print(f"ğŸ“– æ­£åœ¨æå–: {url}")
        try:
            browser_config = BrowserConfig(headless=True)
            md_generator = DefaultMarkdownGenerator(
                options={"citations": True}
            )
            
            run_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=10,
                excluded_tags=["nav", "footer", "header"],
                markdown_generator=md_generator
            )
            
            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=url, config=run_config)
                
                if result and result.markdown_v2:
                    content = result.markdown_v2.fit_markdown
                    if content:
                        print(f"  âœ… æˆåŠŸæå–å†…å®¹ ({len(content)} å­—ç¬¦)")
                        extracted_contents[url] = content
                    else:
                        print(f"  âš ï¸  å†…å®¹ä¸ºç©º")
                else:
                    print(f"  âŒ æå–å¤±è´¥")
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {str(e)}")
        print()
    
    # ä¿å­˜ç»“æœ
    print("=" * 80)
    print("ä¿å­˜ç»“æœ")
    print("=" * 80)
    
    # ä¿å­˜æœç´¢ç»“æœ
    search_file = "google_api_search_results.json"
    with open(search_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜: {search_file}")
    
    # ä¿å­˜æå–çš„å†…å®¹
    if extracted_contents:
        content_file = "google_api_guide.md"
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write("# å¦‚ä½•è·å¾— Google API Key å’Œ CSE ID\n\n")
            f.write("## æœç´¢ç»“æœæ±‡æ€»\n\n")
            
            for query, results in all_results.items():
                f.write(f"### æŸ¥è¯¢: {query}\n\n")
                for i, result in enumerate(results, 1):
                    f.write(f"{i}. **{result['title']}**\n")
                    f.write(f"   - URL: {result['link']}\n")
                    f.write(f"   - æ‘˜è¦: {result['snippet']}\n\n")
            
            f.write("\n---\n\n")
            f.write("## è¯¦ç»†å†…å®¹æå–\n\n")
            
            for url, content in extracted_contents.items():
                f.write(f"### æ¥æº: {url}\n\n")
                f.write(content)
                f.write("\n\n---\n\n")
        
        print(f"ğŸ’¾ è¯¦ç»†æŒ‡å—å·²ä¿å­˜: {content_file}")
    
    print()
    print("=" * 80)
    print("âœ… æœç´¢å®Œæˆï¼")
    print("=" * 80)
    print()
    print("ğŸ“Š æ‘˜è¦:")
    print(f"  - æ€»æŸ¥è¯¢æ•°: {len(queries)}")
    print(f"  - æ€»ç»“æœæ•°: {sum(len(r) for r in all_results.values())}")
    print(f"  - æå–å†…å®¹æ•°: {len(extracted_contents)}")


if __name__ == "__main__":
    asyncio.run(search_google_api_setup())
