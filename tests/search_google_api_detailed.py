#!/usr/bin/env python
"""
ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯æœç´¢ Google API é…ç½®æŒ‡å—
"""

import asyncio
import json
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from search import SearchManager
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from crawl4ai import CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator


async def search_google_api_detailed():
    """è¯¦ç»†æœç´¢ Google API é…ç½®"""
    
    print("=" * 80)
    print("æœç´¢ï¼šGoogle Custom Search API é…ç½®æ•™ç¨‹")
    print("=" * 80)
    print()
    
    search_manager = SearchManager()
    
    # æ›´å…·ä½“çš„æœç´¢æŸ¥è¯¢
    queries = [
        "Google Cloud Console Custom Search API key tutorial",
        "get Google Custom Search Engine ID programmable search",
        "Google CSE API credentials setup guide"
    ]
    
    all_results = {}
    best_urls = []
    
    for query in queries:
        print(f"ğŸ” æœç´¢: '{query}'")
        print("-" * 80)
        
        try:
            results = await search_manager.search(
                query,
                num_results=5,
                engine="duckduckgo"
            )
            
            if results:
                print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ\n")
                
                for i, result in enumerate(results, 1):
                    print(f"  {i}. {result['title']}")
                    print(f"     URL: {result['link']}")
                    snippet = result['snippet'][:120] + "..." if len(result['snippet']) > 120 else result['snippet']
                    print(f"     æ‘˜è¦: {snippet}")
                    print()
                    
                    # æ”¶é›†çœ‹èµ·æ¥æœ‰ç”¨çš„URL
                    url = result['link']
                    if any(keyword in url.lower() for keyword in ['tutorial', 'guide', 'setup', 'how-to', 'docs', 'documentation']):
                        if url not in best_urls:
                            best_urls.append(url)
                
                all_results[query] = results
            else:
                print("âŒ æœªæ‰¾åˆ°ç»“æœ\n")
                
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {str(e)}\n")
    
    # ä¿å­˜æœç´¢ç»“æœ
    search_file = "google_api_search_results.json"
    with open(search_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_file}")
    
    # æå–æœ€æœ‰ç”¨çš„URLå†…å®¹
    if best_urls:
        print()
        print("=" * 80)
        print(f"æ‰¾åˆ° {len(best_urls)} ä¸ªå¯èƒ½æœ‰ç”¨çš„æ•™ç¨‹URL")
        print("=" * 80)
        print()
        
        extracted = {}
        
        for i, url in enumerate(best_urls[:3], 1):  # åªæå–å‰3ä¸ª
            print(f"{i}. æ­£åœ¨æå–: {url}")
            try:
                browser_config = BrowserConfig(headless=True)
                md_generator = DefaultMarkdownGenerator(
                    options={"citations": True}
                )
                
                run_config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    word_count_threshold=10,
                    excluded_tags=["nav", "footer", "header", "aside"],
                    markdown_generator=md_generator
                )
                
                async with AsyncWebCrawler(config=browser_config) as crawler:
                    result = await crawler.arun(url=url, config=run_config)
                    
                    if result and result.markdown_v2:
                        content = result.markdown_v2.fit_markdown or result.markdown_v2.markdown_with_citations
                        if content and len(content) > 100:
                            print(f"   âœ… æˆåŠŸæå– {len(content)} å­—ç¬¦")
                            extracted[url] = content
                        else:
                            print(f"   âš ï¸  å†…å®¹å¤ªå°‘æˆ–ä¸ºç©º")
                    else:
                        print(f"   âŒ æå–å¤±è´¥")
            except Exception as e:
                print(f"   âŒ é”™è¯¯: {str(e)}")
            print()
        
        # åˆ›å»ºæŒ‡å—æ–‡æ¡£
        if extracted:
            guide_file = "google_api_setup_guide.md"
            with open(guide_file, 'w', encoding='utf-8') as f:
                f.write("# Google Custom Search API é…ç½®æŒ‡å—\n\n")
                f.write("## æ¦‚è¿°\n\n")
                f.write("æœ¬æŒ‡å—æ•´åˆäº†å¤šä¸ªæ¥æºçš„ä¿¡æ¯ï¼Œå¸®åŠ©æ‚¨è·å– Google Custom Search API Key å’Œ CSE IDã€‚\n\n")
                f.write("---\n\n")
                
                f.write("## æœç´¢ç»“æœæ±‡æ€»\n\n")
                for query, results in all_results.items():
                    f.write(f"### æŸ¥è¯¢: {query}\n\n")
                    for i, result in enumerate(results, 1):
                        f.write(f"{i}. **[{result['title']}]({result['link']})**\n")
                        f.write(f"   {result['snippet']}\n\n")
                
                f.write("\n---\n\n")
                f.write("## è¯¦ç»†æ•™ç¨‹å†…å®¹\n\n")
                
                for i, (url, content) in enumerate(extracted.items(), 1):
                    f.write(f"### æ•™ç¨‹ {i}: {url}\n\n")
                    f.write(content[:3000])  # é™åˆ¶æ¯ä¸ªå†…å®¹çš„é•¿åº¦
                    if len(content) > 3000:
                        f.write("\n\n...(å†…å®¹å·²æˆªæ–­)...\n")
                    f.write("\n\n---\n\n")
                
                # æ·»åŠ å¿«é€Ÿæ­¥éª¤æ€»ç»“
                f.write("## å¿«é€Ÿæ­¥éª¤æ€»ç»“\n\n")
                f.write("æ ¹æ®æœç´¢ç»“æœï¼Œè·å– Google Custom Search API çš„ä¸€èˆ¬æ­¥éª¤å¦‚ä¸‹ï¼š\n\n")
                f.write("### è·å– API Key:\n\n")
                f.write("1. è®¿é—® [Google Cloud Console](https://console.cloud.google.com/)\n")
                f.write("2. åˆ›å»ºæ–°é¡¹ç›®æˆ–é€‰æ‹©ç°æœ‰é¡¹ç›®\n")
                f.write("3. å¯ç”¨ Custom Search API\n")
                f.write("4. è½¬åˆ° **APIs & Services** > **Credentials**\n")
                f.write("5. ç‚¹å‡» **Create Credentials** > **API Key**\n")
                f.write("6. å¤åˆ¶ç”Ÿæˆçš„ API Key\n\n")
                f.write("### è·å– CSE ID (Custom Search Engine ID):\n\n")
                f.write("1. è®¿é—® [Google Programmable Search Engine](https://programmablesearchengine.google.com/)\n")
                f.write("2. ç‚¹å‡» **Get Started** æˆ– **New Search Engine**\n")
                f.write("3. é…ç½®æœç´¢å¼•æ“:\n")
                f.write("   - æŒ‡å®šè¦æœç´¢çš„ç½‘ç«™ï¼ˆæˆ–é€‰æ‹©æœç´¢æ•´ä¸ªç½‘ç»œï¼‰\n")
                f.write("   - è®¾ç½®è¯­è¨€å’Œåœ°åŒº\n")
                f.write("   - ç»™æœç´¢å¼•æ“å‘½å\n")
                f.write("4. åˆ›å»ºåï¼Œè¿›å…¥æœç´¢å¼•æ“çš„æ§åˆ¶é¢æ¿\n")
                f.write("5. åœ¨ **Setup** æˆ– **Overview** é¡µé¢æ‰¾åˆ° **Search engine ID (cx)**\n")
                f.write("6. å¤åˆ¶è¿™ä¸ª IDï¼Œè¿™å°±æ˜¯æ‚¨çš„ CSE ID\n\n")
                f.write("### é…ç½® Crawl4AI MCP Server:\n\n")
                f.write("ç¼–è¾‘ `config.json` æ–‡ä»¶:\n\n")
                f.write("```json\n")
                f.write("{\n")
                f.write('  "google": {\n')
                f.write('    "api_key": "your-api-key-here",\n')
                f.write('    "cse_id": "your-cse-id-here"\n')
                f.write("  }\n")
                f.write("}\n")
                f.write("```\n\n")
                f.write("### æ³¨æ„äº‹é¡¹:\n\n")
                f.write("- API Key éœ€è¦å¦¥å–„ä¿ç®¡ï¼Œä¸è¦æ³„éœ²\n")
                f.write("- å…è´¹ç‰ˆ Custom Search API æ¯å¤©æœ‰é…é¢é™åˆ¶ï¼ˆé€šå¸¸æ˜¯ 100 æ¬¡æŸ¥è¯¢/å¤©ï¼‰\n")
                f.write("- å¦‚éœ€æ›´å¤šæŸ¥è¯¢æ¬¡æ•°ï¼Œéœ€è¦è®¾ç½®è®¡è´¹è´¦æˆ·\n")
                f.write("- CSE å¯ä»¥é…ç½®ä¸ºæœç´¢ç‰¹å®šç½‘ç«™æˆ–æ•´ä¸ªäº’è”ç½‘\n")
            
            print(f"ğŸ’¾ é…ç½®æŒ‡å—å·²ä¿å­˜åˆ°: {guide_file}")
            print()
            print("=" * 80)
            print("âœ… æœç´¢å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŒ‡å—æ–‡ä»¶ã€‚")
            print("=" * 80)
        else:
            print("\nâš ï¸  æœªèƒ½æå–åˆ°æœ‰ç”¨çš„æ•™ç¨‹å†…å®¹ï¼Œä½†æœç´¢ç»“æœå·²ä¿å­˜ã€‚")
    else:
        print("\nâš ï¸  æœªæ‰¾åˆ°æ˜æ˜¾çš„æ•™ç¨‹URLï¼Œè¯·æŸ¥çœ‹æœç´¢ç»“æœJSONæ–‡ä»¶ã€‚")
    
    print()
    print("ğŸ“Š ç»Ÿè®¡:")
    print(f"  - æœç´¢æŸ¥è¯¢æ•°: {len(queries)}")
    print(f"  - æ€»ç»“æœæ•°: {sum(len(r) for r in all_results.values())}")
    print(f"  - æå–æ•™ç¨‹æ•°: {len(extracted) if 'extracted' in locals() else 0}")


if __name__ == "__main__":
    asyncio.run(search_google_api_detailed())
